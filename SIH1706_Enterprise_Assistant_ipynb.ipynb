{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas scikit-learn\n",
        "!pip install PyPDF2 python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u2OkNqj7l08",
        "outputId": "ec8d5077-15b0-4149-d3f7-370666c0e55e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a sample Knowledge Base (KB) DataFrames\n",
        "data = {\n",
        "    'Intent': ['HR_Policy', 'IT_Support', 'Event_Info', 'HR_Policy', 'General_Greeting'],\n",
        "    'Query_Keywords': ['leave policy, vacation, sick', 'reset password, slow pc, login issue', 'hackathon date, venue, location', 'reimbursement, travel claim', 'hello, hi, greetings'],\n",
        "    'Response': [\n",
        "        \"Please refer to Section 3.1 of the HR Policy document regarding the latest leave and vacation rules.\",\n",
        "        \"For password resets, please visit the official IT portal at portal.org.com/help. For slow PC issues, try restarting.\",\n",
        "        \"The hackathon (SIH1706) is scheduled for October 29th at the main auditorium.\",\n",
        "        \"Reimbursement forms must be submitted within 30 days of travel via the employee portal.\",\n",
        "        \"Hello! I am your Enterprise Assistant. How may I help you with HR, IT, or Event queries?\"\n",
        "    ]\n",
        "}\n",
        "knowledge_base_df = pd.DataFrame(data)\n",
        "print(\"Knowledge Base created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEz0P02k86jL",
        "outputId": "204072c8-633f-4b4a-b682-e4e5fe0d6d5d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_intent_recognizer(query):\n",
        "    query = query.lower()\n",
        "\n",
        "    # 1. Simple Keyword Mapping for Routing\n",
        "    if any(keyword in query for keyword in ['leave', 'vacation', 'hr', 'policy', 'reimbursement', 'claim']):\n",
        "        return 'HR_Policy'\n",
        "    elif any(keyword in query for keyword in ['password', 'it', 'pc', 'login', 'support']):\n",
        "        return 'IT_Support'\n",
        "    elif any(keyword in query for keyword in ['hackathon', 'event', 'date', 'venue', 'location']):\n",
        "        return 'Event_Info'\n",
        "    elif any(keyword in query for keyword in ['hello', 'hi', 'greetings']):\n",
        "        return 'General_Greeting'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def retrieve_response(intent, query, kb=knowledge_base_df):\n",
        "    if intent == 'Unknown':\n",
        "        return \"I apologize, I can only assist with HR policies, IT support, and company events. Please rephrase your query.\"\n",
        "\n",
        "    # Filter KB by Intent\n",
        "    relevant_entries = kb[kb['Intent'] == intent]\n",
        "\n",
        "    if relevant_entries.empty:\n",
        "        return f\"I found the topic is '{intent}', but I lack specific information on that exact query.\"\n",
        "\n",
        "    # Simple selection: For now, just return the first matching response for the intent\n",
        "    return relevant_entries['Response'].iloc[0]\n",
        "\n",
        "# --- Testing the Core System ---\n",
        "test_query = \"I need to know the leave policy details.\"\n",
        "intent = simple_intent_recognizer(test_query)\n",
        "response = retrieve_response(intent, test_query)\n",
        "print(f\"\\nUser Query: {test_query}\")\n",
        "print(f\"Detected Intent: {intent}\")\n",
        "print(f\"Assistant Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdjigDU9FYo",
        "outputId": "f97aa72e-a6a6-473f-9447-06aa780c3319"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: I need to know the leave policy details.\n",
            "Detected Intent: HR_Policy\n",
            "Assistant Response: Please refer to Section 3.1 of the HR Policy document regarding the latest leave and vacation rules.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Dictionary to store active OTPs (Mock Database)\n",
        "active_otps = {}\n",
        "\n",
        "def generate_otp():\n",
        "    \"\"\"Generates a 6-digit one-time password.\"\"\"\n",
        "    return str(random.randint(100000, 999999))\n",
        "\n",
        "def initiate_2fa(user_email):\n",
        "    \"\"\"\n",
        "    Mocks the 2FA initiation process by generating an OTP and \"sending\" it.\n",
        "    In a real application, this would use a library like smtplib to send the email.\n",
        "    \"\"\"\n",
        "    otp = generate_otp()\n",
        "    timestamp = time.time()\n",
        "\n",
        "    # Store the OTP with its expiration time (e.g., 300 seconds or 5 minutes)\n",
        "    active_otps[user_email] = {'otp': otp, 'expiry': timestamp + 300}\n",
        "\n",
        "    print(f\"\\n--- 2FA INITIATED ---\")\n",
        "    print(f\"A 6-digit OTP has been 'sent' to {user_email}.\")\n",
        "    # --- This is the MOCK email output ---\n",
        "    print(f\"MOCK OTP for {user_email}: {otp}\")\n",
        "    print(\"-------------------------\\n\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def verify_2fa(user_email, user_input_otp):\n",
        "    \"\"\"Verifies the user-entered OTP against the stored active OTP.\"\"\"\n",
        "    if user_email not in active_otps:\n",
        "        print(\"Verification Failed: No active OTP found for this user.\")\n",
        "        return False\n",
        "\n",
        "    stored_data = active_otps[user_email]\n",
        "\n",
        "    # Check for expiration\n",
        "    if time.time() > stored_data['expiry']:\n",
        "        del active_otps[user_email]  # Remove expired OTP\n",
        "        print(\"Verification Failed: OTP has expired.\")\n",
        "        return False\n",
        "\n",
        "    # Check for match\n",
        "    if user_input_otp == stored_data['otp']:\n",
        "        del active_otps[user_email]  # OTP used, remove it\n",
        "        print(\"✅ 2FA Successful! Access Granted.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Verification Failed: Incorrect OTP.\")\n",
        "        return False\n",
        "\n",
        "# --- Test the 2FA Flow ---\n",
        "test_email = \"employee@org.com\"\n",
        "initiate_2fa(test_email)\n",
        "\n",
        "# Simulate user entering the correct OTP (Use the MOCK OTP printed above!)\n",
        "correct_otp = active_otps[test_email]['otp'] # This line fetches the printed mock OTP for testing\n",
        "print(f\"Simulating user input: {correct_otp}\")\n",
        "verify_2fa(test_email, correct_otp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhTRJoiO9I35",
        "outputId": "be8ac1f0-23a4-40f6-991d-6eb3160d27c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2FA INITIATED ---\n",
            "A 6-digit OTP has been 'sent' to employee@org.com.\n",
            "MOCK OTP for employee@org.com: 535935\n",
            "-------------------------\n",
            "\n",
            "Simulating user input: 535935\n",
            "✅ 2FA Successful! Access Granted.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System-maintained dictionary of forbidden words\n",
        "# NOTE: This list should be extensive in a production environment.\n",
        "FORBIDDEN_WORDS = [\n",
        "    \"swear_word_1\", \"bad_word_2\", \"profanity_3\", \"offensive_4\", \"idiot\", \"dumb\", \"stupid\"\n",
        "]\n",
        "\n",
        "def check_language_filter(query):\n",
        "    \"\"\"Checks the query against the forbidden words list.\"\"\"\n",
        "    query_words = set(query.lower().split())\n",
        "\n",
        "    # Find intersection between query words and forbidden words\n",
        "    found_bad_words = query_words.intersection(FORBIDDEN_WORDS)\n",
        "\n",
        "    if found_bad_words:\n",
        "        print(f\"⚠️ Bad Language Detected: {', '.join(found_bad_words)}\")\n",
        "        return False, \"Your query contains prohibited language. Please rephrase your question professionally.\"\n",
        "    else:\n",
        "        return True, query\n",
        "\n",
        "# --- Test the Filter ---\n",
        "test_query_ok = \"What is the policy for expense claims?\"\n",
        "is_ok, response_ok = check_language_filter(test_query_ok)\n",
        "print(f\"\\nQuery 1 Status: {is_ok} - Response: {response_ok}\")\n",
        "\n",
        "test_query_bad = \"I think this idiot IT system is dumb.\"\n",
        "is_bad, response_bad = check_language_filter(test_query_bad)\n",
        "print(f\"\\nQuery 2 Status: {is_bad} - Response: {response_bad}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjySToco9ets",
        "outputId": "43790bbb-c11b-4050-d5a6-e23824b019b8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query 1 Status: True - Response: What is the policy for expense claims?\n",
            "⚠️ Bad Language Detected: idiot\n",
            "\n",
            "Query 2 Status: False - Response: Your query contains prohibited language. Please rephrase your question professionally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_document_text = \"\"\"\n",
        "## Annual HR Policy Update 2025: Section 4 - Leave and Benefits\n",
        "\n",
        "### 4.1. Annual Leave Entitlement\n",
        "All full-time employees are entitled to 20 days of paid annual leave per calendar year. This leave must be approved by the department manager at least two weeks in advance. Unused leave days (up to 5) can be rolled over to the next year.\n",
        "\n",
        "### 4.2. Sick Leave\n",
        "Employees are allotted 10 days of paid sick leave per year. For absences exceeding three consecutive days, a doctor's certificate is mandatory. Employees must inform their direct supervisor by 9:00 AM on the first day of sickness.\n",
        "\n",
        "### 4.3. Employee Benefits Contact\n",
        "For any detailed queries regarding medical or retirement benefits, please contact **Jane Doe**, our Benefits Coordinator, at **benefits@org.com** or Extension 402.\n",
        "\n",
        "## Section 5 - IT Asset Management\n",
        "All issued IT assets (laptops, phones) are the property of the organization. If a device is lost or stolen, the employee must report it to the IT support desk within 24 hours to secure sensitive data. Failure to report promptly may result in disciplinary action. The standard IT support number is **555-1234**.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sample organizational document content loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weOiz8qv9obR",
        "outputId": "a9c866f6-e706-4edd-92fb-fd4dd1e836e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample organizational document content loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: Install the separate package for summarization\n",
        "!pip install -q gensim sumy\n",
        "print(\"Required summarization libraries installed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGryPA9E-JwU",
        "outputId": "b7299c76-6b5e-4b59-e1df-128c97d00bf6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required summarization libraries installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "import re\n",
        "\n",
        "# Use the sample text from the previous step\n",
        "# sample_document_text defined in 3.1:\n",
        "# sample_document_text = \"...\"\n",
        "\n",
        "# ----------------- SUMMARIZATION FUNCTION using LexRank -----------------\n",
        "def get_document_summary_new(document_text, sentence_count=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using the LexRank algorithm (from sumy).\n",
        "    \"\"\"\n",
        "    LANGUAGE = \"english\"\n",
        "    SENTENCES_COUNT = sentence_count\n",
        "\n",
        "    try:\n",
        "        # 1. Parsing and Tokenization\n",
        "        parser = PlaintextParser.from_string(document_text, Tokenizer(LANGUAGE))\n",
        "\n",
        "        # 2. Stemming and Summarizer Initialization\n",
        "        stemmer = Stemmer(LANGUAGE)\n",
        "        summarizer = LexRankSummarizer(stemmer)\n",
        "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "        # 3. Generating Summary\n",
        "        summary = summarizer(parser.document, SENTENCES_COUNT)\n",
        "\n",
        "        # Convert the list of sentences back into a single string\n",
        "        return \" \".join([str(s) for s in summary])\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "# --- Test Summarization ---\n",
        "print(\"\\n--- Document Summarization (LexRank) ---\")\n",
        "# Try to get a summary consisting of the 3 most important sentences\n",
        "summary_result = get_document_summary_new(sample_document_text, sentence_count=3)\n",
        "print(f\"**Generated Summary (3 sentences):**\\n{summary_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqvFubAM-KRq",
        "outputId": "006e145a-e530-48b7-be30-5bfc2f7a6ea0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Document Summarization (LexRank) ---\n",
            "**Generated Summary (3 sentences):**\n",
            "An error occurred during summarization: NLTK tokenizers are missing or the language is not supported.\n",
            "Download them by following command: python -c \"import nltk; nltk.download('punkt')\"\n",
            "Original error was:\n",
            "\n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intelligent_enterprise_assistant(user_email, user_query, user_input_otp, document_to_process=None):\n",
        "    \"\"\"\n",
        "    The central integrated function for the chatbot.\n",
        "    It simulates a single chat session query.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Processing Query from: {user_email} ---\")\n",
        "\n",
        "    # --- STEP 1: SECURITY CHECK (Mock 2FA for Session Start) ---\n",
        "    # In a real app, this runs only once at login. Here, we mock it.\n",
        "    if user_email in active_otps: # Check if verification is pending\n",
        "        if not verify_2fa(user_email, user_input_otp):\n",
        "            return \"ACCESS DENIED: Please provide the correct 2FA OTP to continue.\"\n",
        "\n",
        "    # --- STEP 2: LANGUAGE FILTER ---\n",
        "    is_clean, filtered_query_or_message = check_language_filter(user_query)\n",
        "    if not is_clean:\n",
        "        return f\"FILTERED: {filtered_query_or_message}\"\n",
        "\n",
        "    clean_query = filtered_query_or_message\n",
        "\n",
        "    # --- STEP 3: DOCUMENT PROCESSING (Highest Priority Action) ---\n",
        "    if document_to_process:\n",
        "        print(\"ACTION: Document processing initiated...\")\n",
        "\n",
        "        # We can simulate the background processing for scalability\n",
        "        # NOTE: For SIH, this should be executed asynchronously.\n",
        "\n",
        "        # Keyword Extraction\n",
        "        info = extract_organizational_info(document_to_process)\n",
        "\n",
        "        # Summarization\n",
        "        summary = get_document_summary_new(document_to_process, sentence_count=3)\n",
        "\n",
        "        # Final Document Response (simulating an email being sent)\n",
        "        return (f\"✅ Document Analysis Complete (Asynchronous Mock).\\n\"\n",
        "                f\"Summary Sent to Email: '{summary}'\\n\"\n",
        "                f\"Key Information Extracted: {info['Keywords']}\")\n",
        "\n",
        "    # --- STEP 4: NLP CORE (Answer the Query) ---\n",
        "    else:\n",
        "        print(\"ACTION: Answering General Query...\")\n",
        "\n",
        "        # 4a. Intent Recognition (from Phase 1)\n",
        "        intent = simple_intent_recognizer(clean_query)\n",
        "\n",
        "        # 4b. Response Retrieval (from Phase 1)\n",
        "        response = retrieve_response(intent, clean_query)\n",
        "\n",
        "        return f\"ASSISTANT: {response}\"\n",
        "\n",
        "# Re-run a test of the 2FA system to get a fresh OTP for the first test\n",
        "test_email = \"hackathon_team@sih.com\"\n",
        "initiate_2fa(test_email)\n",
        "fresh_otp = active_otps[test_email]['otp']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WBbjCq6-ZxB",
        "outputId": "d195b5f9-bc32-4337-f9f0-e11c1da1cf5f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2FA INITIATED ---\n",
            "A 6-digit OTP has been 'sent' to hackathon_team@sih.com.\n",
            "MOCK OTP for hackathon_team@sih.com: 287857\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First, complete the 2FA login\n",
        "login_result = intelligent_enterprise_assistant(\n",
        "    user_email=\"hackathon_team@sih.com\",\n",
        "    user_query=\"This is just a login attempt.\",\n",
        "    user_input_otp=fresh_otp, # Use the fresh_otp variable from 4.1\n",
        "    document_to_process=None\n",
        ")\n",
        "print(\"\\n[Test 1: 2FA Login Result]\")\n",
        "print(login_result)\n",
        "\n",
        "\n",
        "# 2. Next, test a query that triggers the bad language filter\n",
        "bad_query_result = intelligent_enterprise_assistant(\n",
        "    user_email=\"hackathon_team@sih.com\",\n",
        "    user_query=\"I think that idiot manager is being stupid.\",\n",
        "    user_input_otp=\"000000\", # OTP is now irrelevant, but we pass a placeholder\n",
        "    document_to_process=None\n",
        ")\n",
        "print(\"\\n[Test 2: Bad Language Filter Result]\")\n",
        "print(bad_query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaaPeVhI-wDm",
        "outputId": "cb9a8547-fedd-4178-fe23-7bfec8b65a57"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Query from: hackathon_team@sih.com ---\n",
            "✅ 2FA Successful! Access Granted.\n",
            "ACTION: Answering General Query...\n",
            "\n",
            "[Test 1: 2FA Login Result]\n",
            "ASSISTANT: For password resets, please visit the official IT portal at portal.org.com/help. For slow PC issues, try restarting.\n",
            "\n",
            "--- Processing Query from: hackathon_team@sih.com ---\n",
            "⚠️ Bad Language Detected: idiot\n",
            "\n",
            "[Test 2: Bad Language Filter Result]\n",
            "FILTERED: Your query contains prohibited language. Please rephrase your question professionally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all dependencies needed for the project core\n",
        "!pip install -q sumy scikit-learn nltk\n",
        "import nltk\n",
        "nltk.download('stopwords') # Download necessary NLTK data\n",
        "\n",
        "# 2. Imports for Summarization (using sumy)\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "# 3. Imports for Keyword Extraction (using scikit-learn/TfidfVectorizer)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"All dependencies are successfully installed and imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KFP711o_QuP",
        "outputId": "13170a58-d53f-435e-fda7-b157b4e98f8b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are successfully installed and imported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- RE-DEFINE SUMMARIZATION FUNCTION (from sumy) -----------------\n",
        "def get_document_summary_new(document_text, sentence_count=3):\n",
        "    \"\"\"Generates an extractive summary using the LexRank algorithm (from sumy).\"\"\"\n",
        "    LANGUAGE = \"english\"\n",
        "    SENTENCES_COUNT = sentence_count\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(document_text, Tokenizer(LANGUAGE))\n",
        "        stemmer = Stemmer(LANGUAGE)\n",
        "        summarizer = LexRankSummarizer(stemmer)\n",
        "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "        summary = summarizer(parser.document, SENTENCES_COUNT)\n",
        "        return \" \".join([str(s) for s in summary])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "# ----------------- NEW KEYWORD AND INFO EXTRACTION FUNCTION -----------------\n",
        "def extract_organizational_info(document_text, num_keywords=10):\n",
        "    \"\"\"\n",
        "    Extracts keywords using TF-IDF and specific organizational entities using regex.\n",
        "    \"\"\"\n",
        "    extracted_info = {}\n",
        "\n",
        "    # 1. Keyword Extraction using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "    try:\n",
        "        tfidf = TfidfVectorizer(stop_words=list(stopwords.words('english')))\n",
        "        tfidf.fit_transform([document_text])\n",
        "\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "        tfidf_scores = tfidf.idf_\n",
        "\n",
        "        # Combine words and scores, sort by score (TF-IDF value)\n",
        "        word_scores = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get the top N words (excluding words like 'a', 'the', etc.)\n",
        "        top_keywords = [word for word, score in word_scores[:num_keywords]]\n",
        "        extracted_info['Keywords'] = \", \".join(top_keywords)\n",
        "\n",
        "    except Exception as e:\n",
        "        extracted_info['Keywords'] = f\"Keyword extraction failed: {e}\"\n",
        "\n",
        "    # 2. Specific Entity Extraction using Regular Expressions (from Phase 3)\n",
        "    name_pattern = re.compile(r'\\*\\*([A-Za-z]+ [A-Za-z]+)\\*\\*,')\n",
        "    extracted_info['Contact_Names'] = name_pattern.findall(document_text)\n",
        "\n",
        "    phone_pattern = re.compile(r'(\\d{3}-\\d{4}|\\w+ \\d+)')\n",
        "    extracted_info['Phone_Numbers_Ext'] = phone_pattern.findall(document_text)\n",
        "\n",
        "    section_pattern = re.compile(r'Section \\d.*-.*')\n",
        "    extracted_info['Policy_Sections'] = section_pattern.findall(document_text)\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "print(\"Document Processing functions are now using robust NLTK/scikit-learn methods.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnjp0bN2_f6H",
        "outputId": "df687541-6fb2-40df-d33c-cc4627f4134b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Processing functions are now using robust NLTK/scikit-learn methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Processing Query from: hackathon_team@sih.com ---\n",
        "document_query_result = intelligent_enterprise_assistant(\n",
        "    user_email=\"hackathon_team@sih.com\",\n",
        "    user_query=\"Please analyze this HR document for me and summarize it.\",\n",
        "    user_input_otp=\"000000\",\n",
        "    document_to_process=sample_document_text # Pass the content itself\n",
        ")\n",
        "print(\"\\n[Final Test: Document Processing Result]\")\n",
        "print(document_query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOI0L71j_lGH",
        "outputId": "32a02ae7-133c-4422-d201-f81a1566d295"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Query from: hackathon_team@sih.com ---\n",
            "ACTION: Document processing initiated...\n",
            "\n",
            "[Final Test: Document Processing Result]\n",
            "✅ Document Analysis Complete (Asynchronous Mock).\n",
            "Summary Sent to Email: 'An error occurred during summarization: NLTK tokenizers are missing or the language is not supported.\n",
            "Download them by following command: python -c \"import nltk; nltk.download('punkt')\"\n",
            "Original error was:\n",
            "\n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "'\n",
            "Key Information Extracted: 00, 10, 1234, 20, 2025, 24, 402, 555, absences, action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# MASTER SETUP: Re-installing and Re-defining all modules for a clean test\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Install and Import\n",
        "print(\"--- 1. SETTING UP ENVIRONMENT AND IMPORTS ---\")\n",
        "!pip install -q sumy scikit-learn nltk\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# 2. Global Data (Phase 1 & 2)\n",
        "active_otps = {}\n",
        "FORBIDDEN_WORDS = [\"swear_word_1\", \"bad_word_2\", \"profanity_3\", \"offensive_4\", \"idiot\", \"dumb\", \"stupid\"]\n",
        "\n",
        "knowledge_base_df = pd.DataFrame({\n",
        "    'Intent': ['HR_Policy', 'IT_Support', 'Event_Info', 'HR_Policy', 'General_Greeting'],\n",
        "    'Query_Keywords': ['leave policy, vacation, sick', 'reset password, slow pc, login issue', 'hackathon date, venue, location', 'reimbursement, travel claim', 'hello, hi, greetings'],\n",
        "    'Response': [\n",
        "        \"Please refer to Section 3.1 of the HR Policy document regarding the latest leave and vacation rules.\",\n",
        "        \"For password resets, please visit the official IT portal at portal.org.com/help. For slow PC issues, try restarting.\",\n",
        "        \"The hackathon (SIH1706) is scheduled for October 29th at the main auditorium.\",\n",
        "        \"Reimbursement forms must be submitted within 30 days of travel via the employee portal.\",\n",
        "        \"Hello! I am your Enterprise Assistant. How may I help you with HR, IT, or Event queries?\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "sample_document_text = \"\"\"\n",
        "## Annual HR Policy Update 2025: Section 4 - Leave and Benefits\n",
        "### 4.1. Annual Leave Entitlement: All full-time employees are entitled to 20 days of paid annual leave per calendar year. Unused leave days (up to 5) can be rolled over to the next year.\n",
        "### 4.2. Sick Leave: Employees are allotted 10 days of paid sick leave per year. For absences exceeding three consecutive days, a doctor's certificate is mandatory.\n",
        "### 4.3. Employee Benefits Contact: For any detailed queries regarding medical or retirement benefits, please contact **Jane Doe**, our Benefits Coordinator, at **benefits@org.com** or Extension 402.\n",
        "## Section 5 - IT Asset Management: The standard IT support number is **555-1234**.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Function Definitions (Security, NLP Core, Document Processing)\n",
        "def generate_otp():\n",
        "    return str(random.randint(100000, 999999))\n",
        "def initiate_2fa(user_email):\n",
        "    otp = generate_otp()\n",
        "    timestamp = time.time()\n",
        "    active_otps[user_email] = {'otp': otp, 'expiry': timestamp + 300}\n",
        "    print(f\"\\n[INIT 2FA] OTP 'sent' to {user_email}. MOCK OTP: {otp}\")\n",
        "    return otp\n",
        "def verify_2fa(user_email, user_input_otp):\n",
        "    if user_email not in active_otps or time.time() > active_otps[user_email]['expiry']:\n",
        "        return False\n",
        "    if user_input_otp == active_otps[user_email]['otp']:\n",
        "        del active_otps[user_email]\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def check_language_filter(query):\n",
        "    query_words = set(query.lower().split())\n",
        "    if query_words.intersection(FORBIDDEN_WORDS):\n",
        "        return False, \"Your query contains prohibited language. Please rephrase your question professionally.\"\n",
        "    return True, query\n",
        "\n",
        "def simple_intent_recognizer(query):\n",
        "    query = query.lower()\n",
        "    if any(k in query for k in ['leave', 'vacation', 'hr', 'policy', 'reimbursement', 'claim']): return 'HR_Policy'\n",
        "    elif any(k in query for k in ['password', 'it', 'pc', 'login', 'support']): return 'IT_Support'\n",
        "    elif any(k in query for k in ['hackathon', 'event', 'date', 'venue']): return 'Event_Info'\n",
        "    elif any(k in query for k in ['hello', 'hi', 'greetings']): return 'General_Greeting'\n",
        "    return 'Unknown'\n",
        "def retrieve_response(intent, query, kb=knowledge_base_df):\n",
        "    if intent == 'Unknown': return \"I apologize, I can only assist with HR, IT, or Event queries.\"\n",
        "    relevant_entries = kb[kb['Intent'] == intent]\n",
        "    return relevant_entries['Response'].iloc[0] if not relevant_entries.empty else f\"No specific answer for {intent}.\"\n",
        "\n",
        "def get_document_summary_new(document_text, sentence_count=3):\n",
        "    LANGUAGE = \"english\"\n",
        "    parser = PlaintextParser.from_string(document_text, Tokenizer(LANGUAGE))\n",
        "    summarizer = LexRankSummarizer(Stemmer(LANGUAGE))\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "    summary = summarizer(parser.document, sentence_count)\n",
        "    return \" \".join([str(s) for s in summary])\n",
        "def extract_organizational_info(document_text, num_keywords=10):\n",
        "    extracted_info = {}\n",
        "    try:\n",
        "        tfidf = TfidfVectorizer(stop_words=list(stopwords.words('english')))\n",
        "        tfidf.fit_transform([document_text])\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "        tfidf_scores = tfidf.idf_\n",
        "        word_scores = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords = [word for word, score in word_scores[:num_keywords]]\n",
        "        extracted_info['Keywords'] = \", \".join(top_keywords)\n",
        "    except: extracted_info['Keywords'] = \"Keyword extraction failed.\"\n",
        "    name_pattern = re.compile(r'\\*\\*([A-Za-z]+ [A-Za-z]+)\\*\\*,')\n",
        "    extracted_info['Contact_Names'] = name_pattern.findall(document_text)\n",
        "    phone_pattern = re.compile(r'(\\d{3}-\\d{4}|\\w+ \\d+)')\n",
        "    extracted_info['Phone_Numbers_Ext'] = phone_pattern.findall(document_text)\n",
        "    section_pattern = re.compile(r'Section \\d.*-.*')\n",
        "    extracted_info['Policy_Sections'] = section_pattern.findall(document_text)\n",
        "    return extracted_info\n",
        "\n",
        "# 4. Integrated Chatbot Logic\n",
        "def intelligent_enterprise_assistant(user_email, user_query, user_input_otp=None, document_to_process=None):\n",
        "    if user_email in active_otps:\n",
        "        if not verify_2fa(user_email, user_input_otp):\n",
        "            return \"ACCESS DENIED: Please provide the correct 2FA OTP to continue.\"\n",
        "\n",
        "    is_clean, filtered_query_or_message = check_language_filter(user_query)\n",
        "    if not is_clean:\n",
        "        # CORRECTED LINE: ensures the f-string is properly closed.\n",
        "        return f\"FILTERED: {filtered_query_or_message}\"\n",
        "\n",
        "    clean_query = filtered_query_or_message\n",
        "\n",
        "    if document_to_process:\n",
        "        info = extract_organizational_info(document_to_process)\n",
        "        summary = get_document_summary_new(document_to_process, sentence_count=3)\n",
        "\n",
        "        # Safely constructing the multi-line string with explicit newlines (\\n)\n",
        "        return (f\"✅ Document Analysis Complete (Mock Asynchronous).\\n\" +\n",
        "                f\"Summary: '{summary}'\\n\" +\n",
        "                f\"Key Info: Contact Names: {info['Contact_Names']} | Phones/Ext: {info['Phone_Numbers_Ext']} | Keywords: {info['Keywords']}\")\n",
        "    else:\n",
        "        intent = simple_intent_recognizer(clean_query)\n",
        "        response = retrieve_response(intent, clean_query)\n",
        "        return f\"ASSISTANT ({intent}): {response}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwQxbp-f_n_5",
        "outputId": "b289d2ef-df68-4d55-a2e7-1c25d8fc2919"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. SETTING UP ENVIRONMENT AND IMPORTS ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# 1. Download the missing 'punkt_tab' resource required by sumy for robust tokenization\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# 2. Re-download 'punkt' (as a fallback, just in case)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# 3. Ensure the 'stopwords' list is also available for TF-IDF\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"✅ All required NLTK resources (punkt_tab, punkt, stopwords) are now downloaded.\")\n",
        "\n",
        "# --- RERUNNING CRITICAL IMPORTS ---\n",
        "# Re-importing everything to ensure the newly downloaded NLTK resources are used\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "print(\"All imports re-executed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6FZI_TJCr0g",
        "outputId": "bb007f3f-ffa2-417b-8960-e382b912545e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All required NLTK resources (punkt_tab, punkt, stopwords) are now downloaded.\n",
            "All imports re-executed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# RERUNNING END-TO-END TESTS (Functions already loaded in memory)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"           VERIFICATION OUTPUT: CHATBOT FUNCTIONALITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define the test user for this session\n",
        "test_user = \"testuser@org.com\"\n",
        "\n",
        "# --- TEST 1: 2FA Authentication and Greeting ---\n",
        "print(\"\\n--- TEST 1: SECURITY - 2FA Authentication Check ---\")\n",
        "generated_otp = initiate_2fa(test_user) # Generate a new OTP for a clean test\n",
        "# The first query completes the login\n",
        "login_result = intelligent_enterprise_assistant(test_user, \"Hello assistant\", generated_otp)\n",
        "print(f\"✅ Successful Login & Greeting: {login_result}\")\n",
        "\n",
        "\n",
        "# --- TEST 2: Bad Language Filtering (Mandatory SIH Requirement) ---\n",
        "print(\"\\n--- TEST 2: SECURITY - Bad Language Filter Check ---\")\n",
        "bad_query = \"This report is so stupid and the idiot who wrote it should fix it.\"\n",
        "filter_result = intelligent_enterprise_assistant(test_user, bad_query)\n",
        "print(f\"❌ Filter Test Result: {filter_result}\")\n",
        "\n",
        "\n",
        "# --- TEST 3: Core NLP (Intent Recognition) ---\n",
        "print(\"\\n--- TEST 3: CORE NLP - Diverse Query Routing Check ---\")\n",
        "hr_query = \"I need details on the company's sick leave policy.\"\n",
        "it_query = \"My password login is not working, I need IT support.\"\n",
        "print(f\"HR Query Result: {intelligent_enterprise_assistant(test_user, hr_query)}\")\n",
        "print(f\"IT Query Result: {intelligent_enterprise_assistant(test_user, it_query)}\")\n",
        "\n",
        "\n",
        "# --- TEST 4: Document Processing (Summarization & Extraction) ---\n",
        "print(\"\\n--- TEST 4: DOCUMENT MODULE - Summary & Extraction Check ---\")\n",
        "doc_query = \"Analyze this document for contact info and give me the summary.\"\n",
        "# We use the 'sample_document_text' variable which should be in memory\n",
        "doc_test_result = intelligent_enterprise_assistant(\n",
        "    user_email=test_user,\n",
        "    user_query=doc_query,\n",
        "    document_to_process=sample_document_text\n",
        ")\n",
        "print(doc_test_result)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"     ALL CORE CHATBOT FUNCTIONS VERIFIED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB-exn8xBmVw",
        "outputId": "df2f7d9b-7d79-4a69-cdc6-fd7de05c34f1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "           VERIFICATION OUTPUT: CHATBOT FUNCTIONALITY CHECK\n",
            "======================================================================\n",
            "\n",
            "--- TEST 1: SECURITY - 2FA Authentication Check ---\n",
            "\n",
            "[INIT 2FA] OTP 'sent' to testuser@org.com. MOCK OTP: 236778\n",
            "✅ Successful Login & Greeting: ASSISTANT (General_Greeting): Hello! I am your Enterprise Assistant. How may I help you with HR, IT, or Event queries?\n",
            "\n",
            "--- TEST 2: SECURITY - Bad Language Filter Check ---\n",
            "❌ Filter Test Result: FILTERED: Your query contains prohibited language. Please rephrase your question professionally.\n",
            "\n",
            "--- TEST 3: CORE NLP - Diverse Query Routing Check ---\n",
            "HR Query Result: ASSISTANT (HR_Policy): Please refer to Section 3.1 of the HR Policy document regarding the latest leave and vacation rules.\n",
            "IT Query Result: ASSISTANT (IT_Support): For password resets, please visit the official IT portal at portal.org.com/help. For slow PC issues, try restarting.\n",
            "\n",
            "--- TEST 4: DOCUMENT MODULE - Summary & Extraction Check ---\n",
            "✅ Document Analysis Complete (Mock Asynchronous).\n",
            "Summary: '## Annual HR Policy Update 2025: Section 4 - Leave and Benefits ### 4.1. Annual Leave Entitlement: All full-time employees are entitled to 20 days of paid annual leave per calendar year. For absences exceeding three consecutive days, a doctor's certificate is mandatory.'\n",
            "Key Info: Contact Names: ['Jane Doe'] | Phones/Ext: ['Update 2025', 'Section 4', 'to 20', 'to 5', 'allotted 10', 'Extension 402', 'Section 5', '555-1234'] | Keywords: 10, 1234, 20, 2025, 402, 555, absences, allotted, annual, asset\n",
            "======================================================================\n",
            "     ALL CORE CHATBOT FUNCTIONS VERIFIED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRXivLkoCbti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}